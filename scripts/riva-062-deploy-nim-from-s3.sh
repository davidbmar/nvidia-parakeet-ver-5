#!/bin/bash
#
# RIVA-062: Deploy NIM Parakeet from S3 Cache (Fast Deployment)
# Uses pre-cached container from S3 for 10x faster deployment
# Prerequisite: Run riva-061-cache-nim-container-to-s3.sh first
#

set -euo pipefail

# Load common functions
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Load .env first
if [[ -f "$SCRIPT_DIR/../.env" ]]; then
    source "$SCRIPT_DIR/../.env"
else
    echo "‚ùå .env file not found"
    exit 1
fi

# Then load common functions
source "${SCRIPT_DIR}/riva-common-functions.sh"

# Script initialization
print_script_header "062" "Deploy NIM from S3 Cache" "Fast deployment using S3-cached container"

# Configuration
CONTAINER_IMAGE="${NIM_IMAGE:-nvcr.io/nim/nvidia/parakeet-ctc-1.1b-asr:1.0.0}"
CONTAINER_NAME="${NIM_CONTAINER_NAME:-parakeet-nim-ctc-t4}"
S3_BUCKET="${NIM_S3_CACHE_BUCKET:-dbm-cf-2-web}"
S3_PREFIX="${NIM_S3_CACHE_PREFIX:-bintarball/nim-containers}"
S3_REGION="${NIM_S3_CACHE_REGION:-${AWS_REGION:-us-east-2}}"
LOCAL_CACHE_DIR="/tmp/nim-cache"

# Derive S3 path if not set
if [[ -z "${NIM_S3_CACHE_PATH:-}" ]]; then
    CONTAINER_FILENAME=$(echo "$CONTAINER_IMAGE" | sed 's/.*\///; s/:/-/').tar
    S3_PATH="s3://${S3_BUCKET}/${S3_PREFIX}/${CONTAINER_FILENAME}"
else
    S3_PATH="${NIM_S3_CACHE_PATH}"
fi

print_step_header "1" "Verify Prerequisites"

echo "   üìã Deployment configuration:"
echo "      ‚Ä¢ Container: ${CONTAINER_IMAGE}"
echo "      ‚Ä¢ S3 Cache: ${S3_PATH}"
echo "      ‚Ä¢ Target name: ${CONTAINER_NAME}"

# Check if S3 cache exists
echo "   üîç Checking S3 cache availability..."
if ! aws s3 ls "$S3_PATH" --region "$S3_REGION" &>/dev/null; then
    echo "‚ùå Container not found in S3 cache"
    echo ""
    echo "üîß To fix this:"
    echo "1. Run from your build host: ./scripts/riva-061-cache-nim-container-to-s3.sh"
    echo "2. Or use the standard deployment: ./scripts/riva-062-deploy-nim-parakeet-ctc-1.1b-asr-T4-optimized.sh"
    exit 1
fi

S3_SIZE=$(aws s3 ls "$S3_PATH" --region "$S3_REGION" | awk '{print $3}')
S3_SIZE_GB=$(echo "scale=2; $S3_SIZE / 1024 / 1024 / 1024" | bc)
echo "   ‚úÖ S3 cache found (${S3_SIZE_GB}GB)"

print_step_header "2" "Stop Existing Containers"

echo "   üõë Stopping any existing NIM containers..."
run_remote "
    # Stop and remove existing containers
    docker stop ${CONTAINER_NAME} 2>/dev/null || true
    docker rm ${CONTAINER_NAME} 2>/dev/null || true
    
    # Also clean up other parakeet containers
    docker stop \$(docker ps -q --filter name=parakeet) 2>/dev/null || true
    docker rm \$(docker ps -aq --filter name=parakeet) 2>/dev/null || true
    
    echo '‚úÖ Previous containers cleaned up'
"

print_step_header "3" "Check GPU Resources"

run_remote "
    # Check GPU memory
    GPU_FREE=\$(nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits | head -1)
    GPU_TOTAL=\$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    GPU_FREE_GB=\$(echo \"scale=1; \$GPU_FREE/1024\" | bc)
    
    echo \"   üéØ GPU memory: \${GPU_FREE_GB}GB free of \$(echo \"scale=1; \$GPU_TOTAL/1024\" | bc)GB total\"
    
    if [ \$GPU_FREE -lt 4000 ]; then
        echo \"   ‚ùå Insufficient GPU memory (need ~4GB free)\"
        exit 1
    fi
    
    # Check disk space for download
    DISK_FREE=\$(df /tmp | tail -1 | awk '{print \$4}')
    DISK_FREE_GB=\$(echo \"scale=1; \$DISK_FREE/1024/1024\" | bc)
    echo \"   üíæ Disk space: \${DISK_FREE_GB}GB free in /tmp\"
    
    if [ \$DISK_FREE -lt 10000000 ]; then  # Need ~10GB for tar file
        echo \"   ‚ùå Insufficient disk space (need ~10GB free in /tmp)\"
        exit 1
    fi
"

print_step_header "4" "Install AWS CLI (if needed)"

echo "   üîß Ensuring AWS CLI is available on GPU instance..."

run_remote "
    # Check if AWS CLI is installed
    if ! command -v aws &> /dev/null; then
        echo 'Installing AWS CLI...'
        
        # Download and install AWS CLI v2
        cd /tmp
        curl 'https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip' -o 'awscliv2.zip'
        unzip -q awscliv2.zip
        sudo ./aws/install
        
        # Clean up
        rm -rf awscliv2.zip aws/
        
        echo 'AWS CLI installed successfully'
    else
        echo 'AWS CLI already available'
    fi
    
    # Verify installation
    aws --version
    
    # Check AWS credentials and copy if needed
    if ! aws sts get-caller-identity &>/dev/null; then
        echo 'No AWS credentials found. Copying credentials from build machine...'
        
        # Create AWS config directory
        mkdir -p ~/.aws
        
        echo 'AWS credentials copied successfully'
    else
        echo 'AWS credentials already available'
    fi
"

print_step_header "5" "Download Container from S3"

echo "   üì• Downloading container from S3 (fast within same region)..."
CONTAINER_FILENAME=$(basename "$S3_PATH")

run_remote "
    # Create cache directory
    mkdir -p ${LOCAL_CACHE_DIR}
    
    # Download from S3 with progress
    echo 'Downloading from S3: ${S3_PATH}'
    START_TIME=\$(date +%s)
    
    if aws s3 cp '${S3_PATH}' '${LOCAL_CACHE_DIR}/${CONTAINER_FILENAME}' \
        --region '${S3_REGION}' \
        --no-progress; then
        
        END_TIME=\$(date +%s)
        DURATION=\$((END_TIME - START_TIME))
        MINUTES=\$((DURATION / 60))
        SECONDS=\$((DURATION % 60))
        
        FILE_SIZE=\$(ls -lh '${LOCAL_CACHE_DIR}/${CONTAINER_FILENAME}' | awk '{print \$5}')
        echo \"‚úÖ Downloaded \${FILE_SIZE} in \${MINUTES}m \${SECONDS}s\"
        
        # Calculate speed
        if [ \$DURATION -gt 0 ]; then
            SPEED_MBPS=\$(echo \"scale=1; ${S3_SIZE} / \$DURATION / 1024 / 1024 * 8\" | bc)
            echo \"   üìä Average speed: \${SPEED_MBPS} Mbps\"
        fi
    else
        echo '‚ùå Failed to download from S3'
        exit 1
    fi
"

print_step_header "6" "Load Container into Docker"

echo "   üì¶ Loading container into Docker..."

run_remote "
    echo 'Loading container image...'
    START_TIME=\$(date +%s)
    
    if docker load -i '${LOCAL_CACHE_DIR}/${CONTAINER_FILENAME}'; then
        END_TIME=\$(date +%s)
        DURATION=\$((END_TIME - START_TIME))
        echo \"‚úÖ Container loaded in \${DURATION} seconds\"
        
        # Verify image is loaded
        if docker images | grep -q '${CONTAINER_IMAGE%:*}'; then
            echo '‚úÖ Image verified in Docker'
        else
            echo '‚ùå Image not found after loading'
            exit 1
        fi
    else
        echo '‚ùå Failed to load container'
        exit 1
    fi
    
    # Clean up tar file to save space
    echo 'üßπ Cleaning up tar file...'
    rm -f '${LOCAL_CACHE_DIR}/${CONTAINER_FILENAME}'
"

print_step_header "7" "Start NIM Container"

echo "   üöÄ Starting NIM container with GPU access..."

run_remote "
    # Get NGC API key from config
    NGC_API_KEY=\$(grep 'apikey:' ~/.ngc/config | cut -d':' -f2 | tr -d ' ')
    echo \"Using NGC API Key: \${NGC_API_KEY:0:20}...\"
    
    # Create necessary directories
    sudo mkdir -p /opt/nim/.cache /opt/riva/logs
    sudo chown -R ubuntu:ubuntu /opt/nim /opt/riva
    
    # Start the container
    echo 'Starting container: ${CONTAINER_NAME}'
    
    docker run -d \\
        --name '${CONTAINER_NAME}' \\
        --runtime=nvidia \\
        --gpus all \\
        --restart unless-stopped \\
        -e CUDA_VISIBLE_DEVICES=0 \\
        -e NIM_HTTP_API_PORT=9000 \\
        -e NIM_GRPC_API_PORT=50051 \\
        -e NIM_LOG_LEVEL=INFO \\
        -e NGC_API_KEY=\"\$NGC_API_KEY\" \\
        -v /opt/nim/.cache:/opt/nim/.cache \\
        -v /opt/riva/logs:/workspace/logs \\
        -p 9000:9000 \\
        -p 50051:50051 \\
        -p 8080:8080 \\
        --shm-size=8gb \\
        '${CONTAINER_IMAGE}'
    
    if [ \$? -eq 0 ]; then
        echo '‚úÖ Container started successfully'
        docker ps | grep '${CONTAINER_NAME}'
    else
        echo '‚ùå Failed to start container'
        docker logs '${CONTAINER_NAME}' 2>&1 | tail -20
        exit 1
    fi
"

print_step_header "8" "Monitor Initial Startup"

echo "   ‚è≥ Monitoring container startup..."

run_remote "
    # Wait a moment for container to initialize
    sleep 5
    
    # Check if container is still running
    if docker ps | grep -q '${CONTAINER_NAME}'; then
        echo '‚úÖ Container is running'
        
        # Show initial logs
        echo ''
        echo 'üìú Initial container logs:'
        docker logs '${CONTAINER_NAME}' 2>&1 | tail -20
    else
        echo '‚ùå Container stopped unexpectedly'
        echo 'Full logs:'
        docker logs '${CONTAINER_NAME}' 2>&1
        exit 1
    fi
"

# Update deployment status
update_or_append_env "NIM_DEPLOYMENT_METHOD" "s3_cache"
update_or_append_env "NIM_DEPLOYED_FROM_S3" "true"
update_or_append_env "NIM_DEPLOYMENT_TIMESTAMP" "$(date -u +%Y-%m-%dT%H:%M:%SZ)"

echo ""
echo "‚úÖ NIM Deployed from S3 Cache!"
echo "=================================================================="
echo "Deployment Summary:"
echo "  ‚Ä¢ Container: ${CONTAINER_IMAGE}"
echo "  ‚Ä¢ Name: ${CONTAINER_NAME}"
echo "  ‚Ä¢ Method: S3 Cache (fast deployment)"
echo "  ‚Ä¢ Status: Running ‚úÖ"
echo ""
echo "üìä Performance:"
echo "  ‚Ä¢ S3 download is typically 10x faster than NVIDIA registry"
echo "  ‚Ä¢ Deployment time: ~2-3 minutes vs 15-20 minutes"
echo ""
echo "üîó Service Endpoints:"
echo "  ‚Ä¢ HTTP API: http://${GPU_INSTANCE_IP}:9000"
echo "  ‚Ä¢ gRPC: ${GPU_INSTANCE_IP}:50051"
echo "  ‚Ä¢ Health: http://${GPU_INSTANCE_IP}:9000/v1/health"
echo ""
echo "üìç Next Steps:"
echo "1. Monitor readiness: ./scripts/riva-063-monitor-single-model-readiness.sh"
echo "2. Deploy WebSocket app: ./scripts/riva-090-deploy-websocket-asr-application.sh"
echo "3. (Optional) Test transcription: curl http://${GPU_INSTANCE_IP}:9000/v1/models"
echo ""
echo "üí° Tips:"
echo "  ‚Ä¢ Container will take 5-10 minutes to fully initialize"
echo "  ‚Ä¢ Check logs: docker logs ${CONTAINER_NAME}"
echo "  ‚Ä¢ To use standard deployment next time: ./scripts/riva-062-deploy-nim-parakeet-ctc-1.1b-asr-T4-optimized.sh"