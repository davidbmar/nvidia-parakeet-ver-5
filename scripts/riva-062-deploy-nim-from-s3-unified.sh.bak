#!/bin/bash
set -euo pipefail

# Script: riva-062-deploy-nim-from-s3-unified.sh
# Purpose: Unified S3 deployment with interactive container and model selection
# Prerequisites: S3 containers and models cached, NGC credentials configured
# Validation: Selected NIM container running with selected S3-cached models

# Load .env configuration
if [[ -f .env ]]; then
    set -a
    source .env
    set +a
else
    echo "‚ùå .env file not found. Please run setup scripts first."
    exit 1
fi

# Logging functions
log_info() { echo "‚ÑπÔ∏è  $1"; }
log_success() { echo "‚úÖ $1"; }
log_warning() { echo "‚ö†Ô∏è  $1"; }
log_error() { echo "‚ùå $1"; }

log_info "üöÄ RIVA-062: Unified S3 NIM Deployment"
echo "============================================================"
echo "Purpose: Deploy NIM using S3-cached containers and models"
echo "Timestamp: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
echo ""

# Configuration
S3_BUCKET="${NIM_S3_CACHE_BUCKET:-dbm-cf-2-web}"
S3_CONTAINERS_PATH="s3://${S3_BUCKET}/bintarball/nim-containers"
S3_MODELS_PATH="s3://${S3_BUCKET}/bintarball/nim-models"
GPU_HOST="${RIVA_HOST}"

# =============================================================================
# Enhanced Pre-Deployment Resource Validation
# =============================================================================
echo ""
log_info "üöÄ RIVA NIM DEPLOYMENT - Smart Resource Detection"
echo "============================================================"
echo "This script automatically detects your GPU and matches it with"
echo "the right container and model for optimal performance."
echo "============================================================"
echo ""

log_info "üîç Step 1: Detecting GPU Architecture"
echo "====================================="
echo "   üìç Connecting to GPU worker: $GPU_HOST"

GPU_INFO=$(ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no -i ~/.ssh/dbm-sep-12-2025.pem ubuntu@${GPU_HOST} \
    "nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null || echo 'No GPU detected'" 2>/dev/null || echo "Connection failed")

if [[ "$GPU_INFO" == "Connection failed" ]]; then
    log_error "Cannot connect to GPU worker $GPU_HOST"
    echo "   Please ensure:"
    echo "   1. GPU instance is running"
    echo "   2. SSH key is correct: ~/.ssh/dbm-sep-12-2025.pem"
    echo "   3. Security group allows SSH access"
    exit 1
elif [[ "$GPU_INFO" == "No GPU detected" ]]; then
    log_error "No NVIDIA GPU found on worker $GPU_HOST"
    exit 1
else
    echo "   ‚úÖ Detected GPU: $GPU_INFO"

    # Determine GPU architecture
    if [[ "$GPU_INFO" =~ [Tt]4 ]]; then
        GPU_ARCH="t4"
        echo "      üéØ Architecture: T4 (requires T4-optimized containers/models)"
    elif [[ "$GPU_INFO" =~ [Hh]100 ]]; then
        GPU_ARCH="h100"
        echo "      üéØ Architecture: H100 (requires H100-optimized containers/models)"
    else
        GPU_ARCH="unknown"
        echo "      ‚ö†Ô∏è  Unknown GPU architecture. Proceeding with caution..."
    fi
fi

echo ""
log_info "üéØ Step 2: Checking Your Configuration"
echo "====================================="
echo "   üìù Reading deployment targets from your .env file..."

# Parse target configuration from .env
TARGET_CONTAINER="${NIM_S3_CONTAINER_SELECTED:-}"
TARGET_MODEL="${NIM_S3_MODEL_SELECTED:-}"

if [[ -n "$TARGET_CONTAINER" ]] && [[ -n "$TARGET_MODEL" ]]; then
    echo ""
    echo "      üì¶ Target Container: $TARGET_CONTAINER"
    echo "      üß† Target Model: $TARGET_MODEL"
    echo ""

    # Check architecture compatibility
    if [[ "$GPU_ARCH" == "t4" ]]; then
        if [[ "$TARGET_CONTAINER" =~ t4|ctc ]] && [[ "$TARGET_MODEL" =~ t4 ]]; then
            echo "   ‚úÖ Perfect Match: T4 GPU ‚ÜîÔ∏è T4-optimized resources"
            echo "      ‚ö° This will give you the best performance!"
            ARCH_COMPATIBLE=true
        else
            echo "   ‚ùå Compatibility Issue: T4 GPU but wrong resource types"
            echo "      üìù Your .env specifies:"
            echo "         Container: $TARGET_CONTAINER"
            echo "         Model: $TARGET_MODEL"
            echo "      üí° Tip: Update .env with T4-compatible resources"
            ARCH_COMPATIBLE=false
        fi
    elif [[ "$GPU_ARCH" == "h100" ]]; then
        if [[ "$TARGET_CONTAINER" =~ h100 ]] && [[ "$TARGET_MODEL" =~ h100 ]]; then
            echo "   ‚úÖ Perfect Match: H100 GPU ‚ÜîÔ∏è H100-optimized resources"
            echo "      ‚ö° This will give you enterprise-grade performance!"
            ARCH_COMPATIBLE=true
        else
            echo "   ‚ùå Compatibility Issue: H100 GPU but wrong resource types"
            echo "      üìù Your .env specifies:"
            echo "         Container: $TARGET_CONTAINER"
            echo "         Model: $TARGET_MODEL"
            echo "      üí° Tip: Update .env with H100-compatible resources"
            ARCH_COMPATIBLE=false
        fi
    else
        echo "   ‚ö†Ô∏è  Unknown architecture compatibility"
        ARCH_COMPATIBLE=true
    fi
else
    echo ""
    echo "   ‚ÑπÔ∏è  No specific targets found in .env file"
    echo "      üëâ We'll help you choose the best options interactively"
    echo "      ‚ú® Don't worry - we'll match your GPU automatically!"
    ARCH_COMPATIBLE=true
fi

echo ""
log_info "üì¶ Step 3: Checking What's Already Available"
echo "==========================================="
echo "   üîé Looking for existing resources on your GPU worker..."
echo "      (This could save you time if containers are already there!)"
echo ""

echo "   üê≥ Found Docker Images:"
LOCAL_IMAGES=$(ssh -o StrictHostKeyChecking=no -i ~/.ssh/dbm-sep-12-2025.pem ubuntu@${GPU_HOST} \
    "docker images --format '{{.Repository}}:{{.Tag}}' | grep parakeet" 2>/dev/null || echo "")

if [[ -n "$LOCAL_IMAGES" ]]; then
    echo "$LOCAL_IMAGES" | sed 's/^/      ‚Ä¢ /'
else
    echo "      ‚Ä¢ No parakeet containers found"
fi

echo ""
echo "   üß† Found Cached Models:"
LOCAL_MODELS=$(ssh -o StrictHostKeyChecking=no -i ~/.ssh/dbm-sep-12-2025.pem ubuntu@${GPU_HOST} \
    "find /opt/nim-cache -name '*.tar*' -o -name '*parakeet*' -o -name '*model*' -type f 2>/dev/null | head -5" 2>/dev/null || echo "")

if [[ -n "$LOCAL_MODELS" ]]; then
    echo "$LOCAL_MODELS" | sed 's/^/      ‚Ä¢ /'
else
    echo "      ‚Ä¢ No model files found in /opt/nim-cache"
fi
echo ""

# Determine if we have usable local resources
if [[ -z "$LOCAL_IMAGES" ]] && [[ -z "$LOCAL_MODELS" ]]; then
    echo "   ‚ÑπÔ∏è  Local Status: Fresh start - no resources cached yet"
    echo "      üëâ Will download everything from S3 (normal for new setups)"
    LOCAL_RESOURCES_AVAILABLE=false
elif [[ -n "$TARGET_CONTAINER" ]] && [[ -n "$LOCAL_IMAGES" ]] && [[ -n "$TARGET_MODEL" ]] && [[ -n "$LOCAL_MODELS" ]]; then
    # Check if local resources match our targets
    if [[ "$LOCAL_IMAGES" =~ $(echo "$TARGET_CONTAINER" | cut -d. -f1) ]] || [[ "$LOCAL_MODELS" =~ $(basename "$TARGET_MODEL" .tar.gz | cut -d- -f1-3) ]]; then
        echo "   ‚úÖ Local Status: Perfect! Found matching resources for your targets"
        echo "      ‚ö° Will use local resources (fastest deployment)"
        LOCAL_RESOURCES_AVAILABLE=true
    else
        echo "   ‚ö†Ô∏è  Local Status: Found resources but they don't match your .env targets"
        echo "      üìù Your .env wants: $TARGET_CONTAINER + $TARGET_MODEL"
        echo "      üëâ Will download correct resources from S3"
        LOCAL_RESOURCES_AVAILABLE=false
    fi
elif [[ -n "$LOCAL_IMAGES" ]] || [[ -n "$LOCAL_MODELS" ]]; then
    echo "   ‚ÑπÔ∏è  Local Status: Some resources found, checking compatibility..."
    echo "      üëâ Will verify against your .env configuration"
    LOCAL_RESOURCES_AVAILABLE=false
else
    echo "   ‚ÑπÔ∏è  Local Status: No usable resources found"
    echo "      üëâ Will download from S3"
    LOCAL_RESOURCES_AVAILABLE=false
fi

echo ""
log_info "‚òÅÔ∏è  Step 4: Checking S3 Backup Resources"
echo "===================================="
echo "   üì§ Scanning all available resources in your S3 cache..."
echo ""

echo "   üì¶ Available S3 Containers:"
# Get all containers from S3
ALL_S3_CONTAINERS=$(aws s3 ls "${S3_CONTAINERS_PATH}/" --recursive --human-readable | grep -E '\.(tar|tar\.gz)$' || echo "")

if [[ -n "$ALL_S3_CONTAINERS" ]]; then
    S3_CONTAINER_AVAILABLE=false
    while IFS= read -r line; do
        if [[ -n "$line" ]]; then
            container_name=$(echo "$line" | awk '{print $NF}' | xargs basename)
            size=$(echo "$line" | awk '{print $3 " " $4}')

            # Check if this is our target
            if [[ -n "$TARGET_CONTAINER" ]] && [[ "$container_name" == "$TARGET_CONTAINER" ]]; then
                echo "      ‚Ä¢ $container_name ($size) ‚≠ê YOUR TARGET"
                S3_CONTAINER_AVAILABLE=true
            else
                echo "      ‚Ä¢ $container_name ($size)"
            fi
        fi
    done <<< "$ALL_S3_CONTAINERS"

    # Check if target was found
    if [[ -n "$TARGET_CONTAINER" ]] && [[ "$S3_CONTAINER_AVAILABLE" == false ]]; then
        echo "      ‚ùå MISSING: $TARGET_CONTAINER (your .env target)"
    fi
else
    echo "      ‚ùå No containers found in S3"
    S3_CONTAINER_AVAILABLE=false
fi

echo ""
echo "   üß† Available S3 Models:"
# Get all models from S3
ALL_S3_MODELS=$(aws s3 ls "${S3_MODELS_PATH}/" --recursive --human-readable | grep '\.tar\.gz$' || echo "")

if [[ -n "$ALL_S3_MODELS" ]]; then
    S3_MODEL_AVAILABLE=false
    while IFS= read -r line; do
        if [[ -n "$line" ]]; then
            model_name=$(echo "$line" | awk '{print $NF}' | xargs basename)
            size=$(echo "$line" | awk '{print $3 " " $4}')

            # Check if this is our target
            if [[ -n "$TARGET_MODEL" ]] && [[ "$model_name" == "$TARGET_MODEL" ]]; then
                echo "      ‚Ä¢ $model_name ($size) ‚≠ê YOUR TARGET"
                S3_MODEL_AVAILABLE=true
            else
                echo "      ‚Ä¢ $model_name ($size)"
            fi
        fi
    done <<< "$ALL_S3_MODELS"

    # Check if target was found
    if [[ -n "$TARGET_MODEL" ]] && [[ "$S3_MODEL_AVAILABLE" == false ]]; then
        echo "      ‚ùå MISSING: $TARGET_MODEL (your .env target)"
    fi
else
    echo "      ‚ùå No models found in S3"
    S3_MODEL_AVAILABLE=false
fi

# Handle case where no targets specified
if [[ -z "$TARGET_CONTAINER" ]] && [[ -z "$TARGET_MODEL" ]]; then
    echo ""
    echo "   ‚ÑπÔ∏è  No specific targets in .env - will show interactive selection"
    S3_CONTAINER_AVAILABLE=true
    S3_MODEL_AVAILABLE=true
fi

echo ""
log_info "üßê Smart Decision Engine"
echo "========================"
echo "   ü§ñ Analyzing the best deployment path for you..."
echo ""

# Decision logic
if [[ "$ARCH_COMPATIBLE" == false ]]; then
    echo "   ‚ùå STOP: GPU and resource types don't match"
    echo "      üîß Quick fix: Update your .env file with $GPU_ARCH-compatible resources"
    echo "      üí° Or: Choose different resources when prompted"
    exit 1
elif [[ "$LOCAL_RESOURCES_AVAILABLE" == true ]]; then
    echo "   ‚úÖ FAST TRACK: Using resources already on your GPU!"
    echo "      üèÜ This is the fastest option - deployment in under a minute"
    USE_LOCAL_RESOURCES=true
elif [[ "$S3_CONTAINER_AVAILABLE" == true ]] && [[ "$S3_MODEL_AVAILABLE" == true ]]; then
    echo "   ‚úÖ DOWNLOAD MODE: Getting your exact resources from S3"
    echo "      üï∞Ô∏è Estimated time: 3-5 minutes"
    USE_LOCAL_RESOURCES=false
else
    echo "   ‚ùå STOPPING: Required resources not available locally or in S3"
    echo ""
    echo "      Missing resources:"
    [[ "$S3_CONTAINER_AVAILABLE" == false ]] && echo "         - Container: $TARGET_CONTAINER"
    [[ "$S3_MODEL_AVAILABLE" == false ]] && echo "         - Model: $TARGET_MODEL"
    echo ""
    echo "      üìù Required Actions:"
    echo "         1. Cache containers to S3: ./scripts/riva-XXX-cache-containers-to-s3.sh"
    echo "         2. Cache models to S3: ./scripts/riva-XXX-cache-models-to-s3.sh"
    echo "         3. Re-run this deployment script"
    exit 1
fi

echo ""
log_success "PRE-DEPLOYMENT VALIDATION COMPLETE ‚úÖ"
echo "============================================================"
echo ""

# Check if we should use local resources and skip S3 selection
if [[ "$USE_LOCAL_RESOURCES" == true ]]; then
    log_info "üöÄ USING LOCAL RESOURCES - Skipping S3 Selection"
    echo "===================================================="
    echo ""
    echo "   üì¶ Container: $TARGET_CONTAINER (from .env)"
    echo "   üß† Model: $TARGET_MODEL (from .env)"
    echo "   üéØ GPU: $GPU_ARCH architecture detected and compatible"
    echo ""
    log_success "Proceeding directly to deployment with local resources"

    # Set selected resources from .env targets
    SELECTED_CONTAINER="$TARGET_CONTAINER"
    SELECTED_MODEL="$TARGET_MODEL"

    # Jump directly to deployment section
    echo ""
    log_info "üöÄ Starting Local Resource Deployment"
    echo "====================================="

    # Skip to deployment logic by jumping to the deployment section
    # The rest of this script will be the deployment commands

else
    log_info "üì• DOWNLOADING FROM S3 - Resource Selection Required"
    echo "=================================================="
    echo ""

# =============================================================================
# S3 Resource Selection (Only when USE_LOCAL_RESOURCES=false)
# =============================================================================

log_info "üìã Step 1: S3 Cache Overview"
echo "======================================="

echo "üèó S3 ORGANIZED CACHE STRUCTURE:"
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo ""
echo "üì¶ NIM CONTAINERS (GPU-specific):"
echo "================================="
aws s3 ls "${S3_CONTAINERS_PATH}/" --recursive --human-readable | while read line; do
    if [[ "$line" == *"h100-containers"* ]]; then
        echo "  üü¢ H100: $line"
    elif [[ "$line" == *"t4-containers"* ]]; then
        if [[ "$line" == *".tar.gz"* ]] && [[ "$line" != *" 0 Bytes "* ]]; then
            echo "  üü¢ T4: $line"
        elif [[ "$line" == *".tar.gz"* ]]; then
            echo "  ‚è≥ T4: $line (extraction in progress)"
        else
            echo "  üìÅ T4: $line"
        fi
    elif [[ "$line" == *"metadata"* ]]; then
        echo "  üìã META: $line"
    fi
done

echo ""
echo "üß† NIM MODEL CACHES (GPU-optimized):"
echo "===================================="
aws s3 ls "${S3_MODELS_PATH}/" --recursive --human-readable | while read line; do
    if [[ "$line" == *"t4-models"* ]]; then
        echo "  üü¢ T4: $line"
    elif [[ "$line" == *"h100-models"* ]]; then
        echo "  üìÅ H100: $line"
    elif [[ "$line" == *"metadata"* ]]; then
        echo "  üìã META: $line"
    fi
done

echo ""

# =============================================================================
# Step 2: Interactive Container Selection
# =============================================================================
log_info "üìã Step 2: Container Selection"
echo "========================================"

echo "üîç Available S3 Containers:"
echo "==========================="

# Get available containers
declare -a CONTAINERS=()
declare -a CONTAINER_PATHS=()
declare -a CONTAINER_SIZES=()

while IFS= read -r line; do
    if [[ "$line" == *".tar"* ]] && [[ "$line" != *" 0 Bytes "* ]]; then
        # Extract container name and details
        size=$(echo "$line" | awk '{print $3 " " $4}')
        path=$(echo "$line" | awk '{print $NF}')
        container_name=$(basename "$path")
        
        CONTAINERS+=("$container_name")
        CONTAINER_PATHS+=("s3://${S3_BUCKET}/bintarball/$path")
        CONTAINER_SIZES+=("$size")
        
        echo "  [$((${#CONTAINERS[@]}))] $container_name ($size)"
        
        # Add description
        if [[ "$container_name" == *"h100"* ]]; then
            echo "      üéØ H100-optimized, enterprise scale, high throughput"
        elif [[ "$container_name" == *"ctc"* ]]; then
            echo "      ‚ö° T4-optimized, streaming CTC, real-time transcription"
        elif [[ "$container_name" == *"tdt"* ]]; then
            echo "      üéØ T4-optimized, offline TDT, high-accuracy batch"
        fi
        echo ""
    fi
done < <(aws s3 ls "${S3_CONTAINERS_PATH}/" --recursive --human-readable | grep -E "\.(tar|tar\.gz)$")

if [[ ${#CONTAINERS[@]} -eq 0 ]]; then
    log_error "No containers found in S3. Please run container caching scripts first."
    exit 1
fi

echo "Select container:"
while true; do
    read -p "Choice [1-${#CONTAINERS[@]}]: " container_choice
    if [[ "$container_choice" =~ ^[0-9]+$ ]] && [[ "$container_choice" -ge 1 ]] && [[ "$container_choice" -le ${#CONTAINERS[@]} ]]; then
        break
    else
        echo "Please enter a number between 1 and ${#CONTAINERS[@]}"
    fi
done

SELECTED_CONTAINER="${CONTAINERS[$((container_choice-1))]}"
SELECTED_CONTAINER_PATH="${CONTAINER_PATHS[$((container_choice-1))]}"
SELECTED_CONTAINER_SIZE="${CONTAINER_SIZES[$((container_choice-1))]}"

log_success "Selected: $SELECTED_CONTAINER ($SELECTED_CONTAINER_SIZE)"

fi  # End of S3 selection logic

# =============================================================================
# Step 3: Interactive Model Selection (Only for S3 downloads)
# =============================================================================
if [[ "$USE_LOCAL_RESOURCES" != true ]]; then
log_info "üìã Step 3: Model Selection"
echo "========================================"

echo "üîç Available S3 Models:"
echo "======================="

# Get available models
declare -a MODELS=()
declare -a MODEL_PATHS=()
declare -a MODEL_SIZES=()
declare -a MODEL_TYPES=()

while IFS= read -r line; do
    if [[ "$line" == *".tar.gz"* ]]; then
        # Extract model name and details
        size=$(echo "$line" | awk '{print $3 " " $4}')
        path=$(echo "$line" | awk '{print $NF}')
        model_name=$(basename "$path")
        
        # Determine model type
        if [[ "$model_name" == *"ctc"* ]]; then
            model_type="streaming"
        elif [[ "$model_name" == *"tdt"* ]] || [[ "$model_name" == *"offline"* ]]; then
            model_type="offline"
        elif [[ "$model_name" == *"punctuation"* ]]; then
            model_type="enhancement"
        else
            model_type="unknown"
        fi
        
        MODELS+=("$model_name")
        MODEL_PATHS+=("s3://${S3_BUCKET}/bintarball/$path")
        MODEL_SIZES+=("$size")
        MODEL_TYPES+=("$model_type")
        
        echo "  [$((${#MODELS[@]}))] $model_name ($size)"
        
        # Add description
        case "$model_type" in
            "streaming") echo "      ‚ö° Real-time CTC streaming for live transcription" ;;
            "offline") echo "      üéØ High-accuracy TDT for batch processing" ;;
            "enhancement") echo "      ‚ú® Punctuation and formatting enhancement" ;;
        esac
        echo ""
    fi
done < <(aws s3 ls "${S3_MODELS_PATH}/t4-models/" --recursive --human-readable | grep "\.tar\.gz$")

if [[ ${#MODELS[@]} -eq 0 ]]; then
    log_error "No models found in S3. Please run model caching scripts first."
    exit 1
fi

echo "Select primary model:"
while true; do
    read -p "Choice [1-${#MODELS[@]}]: " model_choice
    if [[ "$model_choice" =~ ^[0-9]+$ ]] && [[ "$model_choice" -ge 1 ]] && [[ "$model_choice" -le ${#MODELS[@]} ]]; then
        break
    else
        echo "Please enter a number between 1 and ${#MODELS[@]}"
    fi
done

SELECTED_MODEL="${MODELS[$((model_choice-1))]}"
SELECTED_MODEL_PATH="${MODEL_PATHS[$((model_choice-1))]}"
SELECTED_MODEL_SIZE="${MODEL_SIZES[$((model_choice-1))]}"
SELECTED_MODEL_TYPE="${MODEL_TYPES[$((model_choice-1))]}"

log_success "Selected: $SELECTED_MODEL ($SELECTED_MODEL_SIZE)"

fi  # End of S3 resource selection

# =============================================================================
# Step 4: Deployment Configuration Summary
# =============================================================================
log_info "üìã Step 4: Deployment Configuration"
echo "========================================"

echo "üìä DEPLOYMENT SUMMARY:"
echo "====================="
echo "  üê≥ Container: $SELECTED_CONTAINER"
if [[ "$USE_LOCAL_RESOURCES" == true ]]; then
    echo "      üè† Source: Local resources (.env configuration)"
    echo "      ‚ö° Mode: Direct deployment (fastest)"
else
    echo "      üì¶ Size: $SELECTED_CONTAINER_SIZE"
    echo "      üìç Path: $SELECTED_CONTAINER_PATH"
fi
echo ""
echo "  üß† Model: $SELECTED_MODEL"
if [[ "$USE_LOCAL_RESOURCES" == true ]]; then
    echo "      üè† Source: Local resources (.env configuration)"
    echo "      üéØ Arch: $GPU_ARCH compatible"
else
    echo "      üì¶ Size: $SELECTED_MODEL_SIZE"
    echo "      üéØ Type: $SELECTED_MODEL_TYPE"
    echo "      üìç Path: $SELECTED_MODEL_PATH"
fi
echo ""
echo "  üéõ Target: $GPU_HOST"
if [[ "$USE_LOCAL_RESOURCES" == true ]]; then
    echo "  ‚è± Expected Time: 30-60 seconds (local resources)"
else
    echo "  ‚è± Expected Time: 3-5 minutes (S3 download + deploy)"
fi
echo ""

read -p "Proceed with deployment? [y/N]: " confirm
if [[ ! "$confirm" =~ ^[Yy]$ ]]; then
    log_info "Deployment cancelled"
    exit 0
fi

# =============================================================================
# Step 5: Stop Existing Containers
# =============================================================================
log_info "üìã Step 5: Stop Existing Containers"
echo "========================================"

echo "   üõë Stopping any existing NIM containers..."
ssh -i ~/.ssh/dbm-sep-12-2025.pem ubuntu@${GPU_HOST} \
    "docker stop \$(docker ps -q --filter ancestor=nvcr.io/nim/nvidia/parakeet) 2>/dev/null || true; \
     docker rm \$(docker ps -aq --filter ancestor=nvcr.io/nim/nvidia/parakeet) 2>/dev/null || true"
log_success "Previous containers cleaned up"

# =============================================================================
# Step 6: Download and Load Container from S3
# =============================================================================
log_info "üìã Step 6: Deploy Container from S3"
echo "========================================"

CONTAINER_FILENAME=$(basename "$SELECTED_CONTAINER_PATH")
echo "   üì• Downloading container from S3: $CONTAINER_FILENAME"

ssh -i ~/.ssh/dbm-sep-12-2025.pem ubuntu@${GPU_HOST} "
    mkdir -p /tmp/nim-deploy
    cd /tmp/nim-deploy
    
    echo 'Downloading container from S3...'
    aws s3 cp '$SELECTED_CONTAINER_PATH' ./container.tar.gz
    
    echo 'Loading container into Docker...'
    docker load < container.tar.gz
    
    echo 'Container loaded successfully'
    rm -f container.tar.gz
"

log_success "Container deployed from S3"

# =============================================================================
# Step 7: Download and Extract Model from S3
# =============================================================================
log_info "üìã Step 7: Deploy Model from S3"
echo "========================================"

MODEL_FILENAME=$(basename "$SELECTED_MODEL_PATH")
echo "   üì• Downloading model from S3: $MODEL_FILENAME"

ssh -i ~/.ssh/dbm-sep-12-2025.pem ubuntu@${GPU_HOST} "
    mkdir -p /tmp/nim-models
    cd /tmp/nim-models
    
    echo 'Downloading model cache from S3...'
    aws s3 cp '$SELECTED_MODEL_PATH' ./model-cache.tar.gz
    
    echo 'Extracting model cache...'
    tar -xzf model-cache.tar.gz
    
    echo 'Installing model cache...'
    sudo mkdir -p /opt/nim-cache
    sudo cp -r ngc/* /opt/nim-cache/ 2>/dev/null || cp -r * /opt/nim-cache/
    sudo chown -R 1000:1000 /opt/nim-cache 2>/dev/null || chown -R ubuntu:ubuntu /opt/nim-cache
    
    echo 'Model cache installed successfully'
    rm -f model-cache.tar.gz
"

log_success "Model deployed from S3"

# =============================================================================
# Step 8: Start NIM Container
# =============================================================================
log_info "üìã Step 8: Start NIM Container"
echo "========================================"

# Derive container image name from filename
CONTAINER_NAME="parakeet-nim-s3-unified"
NGC_API_KEY=$(grep 'NGC_API_KEY=' .env | cut -d'=' -f2)

echo "   üöÄ Starting NIM container..."

ssh -i ~/.ssh/dbm-sep-12-2025.pem ubuntu@${GPU_HOST} "
    # Get the loaded image name
    IMAGE_NAME=\$(docker images --format 'table {{.Repository}}:{{.Tag}}' | grep parakeet | head -1)
    echo \"Using image: \$IMAGE_NAME\"
    
    docker run -d \\
        --name $CONTAINER_NAME \\
        --gpus all \\
        --restart unless-stopped \\
        -e NGC_API_KEY='$NGC_API_KEY' \\
        -e NIM_CACHE_PATH=/opt/nim/.cache \\
        -v /opt/nim-cache:/opt/nim/.cache \\
        -p 8080:8080 \\
        -p 9000:9000 \\
        -p 50051:50051 \\
        \$IMAGE_NAME
"

# Verify container started
sleep 5
CONTAINER_STATUS=$(ssh -i ~/.ssh/dbm-sep-12-2025.pem ubuntu@${GPU_HOST} \
    "docker ps --filter name=$CONTAINER_NAME --format '{{.Status}}' | head -1")

if [[ -n "$CONTAINER_STATUS" ]]; then
    log_success "Container started successfully"
    echo "   Status: $CONTAINER_STATUS"
else
    log_error "Container failed to start"
    exit 1
fi

# =============================================================================
# Step 9: Update Environment Configuration
# =============================================================================
log_info "üìã Step 9: Update Configuration"
echo "========================================"

echo "   üìù Updating .env with deployment details..."

# Update .env with selected configuration
cat >> .env << EOF

# ============================================================================
# Unified S3 Deployment Configuration ($(date -u '+%Y-%m-%d %H:%M:%S UTC'))
# ============================================================================
NIM_S3_UNIFIED_DEPLOYMENT=true
NIM_S3_UNIFIED_TIMESTAMP=$(date -u '+%Y-%m-%dT%H:%M:%SZ')
NIM_S3_CONTAINER_SELECTED=$SELECTED_CONTAINER
NIM_S3_CONTAINER_PATH=$SELECTED_CONTAINER_PATH
NIM_S3_CONTAINER_SIZE=$SELECTED_CONTAINER_SIZE
NIM_S3_MODEL_SELECTED=$SELECTED_MODEL
NIM_S3_MODEL_PATH=$SELECTED_MODEL_PATH
NIM_S3_MODEL_SIZE=$SELECTED_MODEL_SIZE
NIM_S3_MODEL_TYPE=$SELECTED_MODEL_TYPE
NIM_DEPLOYMENT_METHOD=s3_unified
NIM_CONTAINER_NAME=$CONTAINER_NAME
EOF

log_success "‚úÖ Unified S3 NIM Deployment Complete!"
echo "=================================================================="
echo "Deployment Summary:"
echo "  üê≥ Container: $SELECTED_CONTAINER ($SELECTED_CONTAINER_SIZE)"
echo "  üß† Model: $SELECTED_MODEL ($SELECTED_MODEL_SIZE)"
echo "  üéØ Method: Unified S3 cached deployment"
echo "  üì¶ Container Name: $CONTAINER_NAME"
echo "  ‚úÖ Status: Running"
echo ""
echo "üîó Service Endpoints:"
echo "  ‚Ä¢ HTTP API: http://${GPU_HOST}:9000"
echo "  ‚Ä¢ gRPC: ${GPU_HOST}:50051"
echo "  ‚Ä¢ Health: http://${GPU_HOST}:9000/v1/health"
echo ""
echo "üìç Next Steps:"
echo "1. Monitor readiness: ./scripts/riva-063-monitor-single-model-readiness.sh"
echo "2. Deploy WebSocket app: ./scripts/riva-090-deploy-websocket-asr-application.sh"
echo "3. Test transcription: curl http://${GPU_HOST}:9000/v1/models"
echo ""
echo "üöÄ Benefits:"
echo "  ‚Ä¢ Complete S3-cached deployment (containers + models)"
echo "  ‚Ä¢ Interactive selection of optimal components"
echo "  ‚Ä¢ 10x faster than fresh NGC downloads"
echo "  ‚Ä¢ GPU-architecture optimized performance"