#!/bin/bash
set -euo pipefail

# Script: riva-144-end-to-end-validation.sh
# Purpose: Comprehensive end-to-end validation of entire WebSocket bridge pipeline
# Prerequisites: riva-143 (client testing) completed
# Validation: Tests complete audio-to-text pipeline with real speech samples

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(cd "$SCRIPT_DIR/.." && pwd)"

source "$SCRIPT_DIR/riva-common-functions.sh"
load_config

log_info "ðŸ” End-to-End WebSocket Bridge Validation"

# Check prerequisites
if [[ ! -f "/opt/riva/nvidia-parakeet-ver-6/.env" ]]; then
    log_error "WebSocket bridge service not found. Run riva-142 first."
    exit 1
fi

source /opt/riva/nvidia-parakeet-ver-6/.env

if [[ "${WS_CLIENT_TESTING_COMPLETE:-false}" != "true" ]]; then
    log_error "Client testing not complete. Run riva-143 first."
    exit 1
fi

log_info "âœ… Prerequisites validated"

# Configuration
WS_HOST="${WS_HOST:-0.0.0.0}"
WS_PORT="${WS_PORT:-8443}"
WS_TLS_ENABLED="${WS_TLS_ENABLED:-false}"
RIVA_HOST="${RIVA_HOST:-localhost}"
RIVA_PORT="${RIVA_PORT:-50051}"

if [[ "$WS_HOST" == "0.0.0.0" ]]; then
    TEST_HOST="localhost"
else
    TEST_HOST="$WS_HOST"
fi

WS_PROTOCOL="ws"
if [[ "${WS_TLS_ENABLED}" == "true" ]]; then
    WS_PROTOCOL="wss"
fi

SERVER_URL="${WS_PROTOCOL}://${TEST_HOST}:${WS_PORT}/"

# Create validation workspace
VALIDATION_DIR="$PROJECT_DIR/validation"
mkdir -p "$VALIDATION_DIR"

log_info "ðŸŒ Server URL: $SERVER_URL"
log_info "ðŸŽ¯ Riva Target: $RIVA_HOST:$RIVA_PORT"

# Validation 1: Infrastructure health check
log_info "ðŸ¥ Validation 1: Infrastructure Health Check"

INFRA_HEALTHY=true

# Check Riva server
if timeout 10 nc -z "$RIVA_HOST" "$RIVA_PORT"; then
    log_success "âœ… Riva server accessible"
else
    log_error "âŒ Riva server not accessible"
    INFRA_HEALTHY=false
fi

# Check WebSocket service
SERVICE_STATUS=$(sudo systemctl is-active riva-websocket-bridge.service || echo "failed")
if [[ "$SERVICE_STATUS" == "active" ]]; then
    log_success "âœ… WebSocket bridge service running"
else
    log_error "âŒ WebSocket bridge service not running"
    INFRA_HEALTHY=false
fi

# Check WebSocket port
if timeout 5 nc -z "$TEST_HOST" "$WS_PORT"; then
    log_success "âœ… WebSocket port accessible"
else
    log_error "âŒ WebSocket port not accessible"
    INFRA_HEALTHY=false
fi

if [[ "$INFRA_HEALTHY" != "true" ]]; then
    log_error "Infrastructure health check failed"
    exit 1
fi

log_success "âœ… Infrastructure health check passed"

# Validation 2: Create comprehensive test audio
log_info "ðŸŽµ Validation 2: Creating comprehensive test audio"

if command -v python3 >/dev/null 2>&1; then
    python3 << 'EOF'
import numpy as np
import wave
import os
import json

# Validation audio parameters
sample_rate = 16000
validation_dir = os.path.join(os.path.dirname(__file__), '..', 'validation')
os.makedirs(validation_dir, exist_ok=True)

# Test scenarios with expected transcriptions
test_scenarios = [
    {
        "name": "simple_phrase",
        "text": "Hello world this is a test",
        "frequencies": [200, 400, 600, 800],  # Speech-like frequencies
        "duration": 3.0,
        "expected_words": ["hello", "world", "test"]
    },
    {
        "name": "numbers",
        "text": "One two three four five",
        "frequencies": [300, 500, 700],
        "duration": 4.0,
        "expected_words": ["one", "two", "three", "four", "five"]
    },
    {
        "name": "technical_terms",
        "text": "Machine learning artificial intelligence",
        "frequencies": [250, 450, 650, 850],
        "duration": 5.0,
        "expected_words": ["machine", "learning", "artificial", "intelligence"]
    }
]

# Create test audio files
for scenario in test_scenarios:
    name = scenario["name"]
    duration = scenario["duration"]
    frequencies = scenario["frequencies"]

    # Generate time array
    t = np.linspace(0, duration, int(sample_rate * duration), False)

    # Create complex audio signal (simulate speech patterns)
    audio = np.zeros_like(t)

    # Add multiple frequency components with modulation
    for i, freq in enumerate(frequencies):
        # Add base frequency
        audio += np.sin(2 * np.pi * freq * t) * (0.2 / len(frequencies))

        # Add harmonics
        audio += np.sin(2 * np.pi * freq * 2 * t) * (0.1 / len(frequencies))

        # Add modulation to simulate speech characteristics
        modulation = 1 + 0.3 * np.sin(2 * np.pi * 5 * t)  # 5 Hz modulation
        audio *= modulation

    # Add some noise to make it more realistic
    noise = np.random.normal(0, 0.05, len(audio))
    audio += noise

    # Apply envelope (fade in/out)
    fade_samples = int(0.1 * sample_rate)  # 100ms fade
    audio[:fade_samples] *= np.linspace(0, 1, fade_samples)
    audio[-fade_samples:] *= np.linspace(1, 0, fade_samples)

    # Normalize and convert to 16-bit PCM
    audio = np.clip(audio, -1, 1)
    audio_int16 = (audio * 32767).astype(np.int16)

    # Save as WAV file
    wav_path = os.path.join(validation_dir, f"{name}.wav")
    with wave.open(wav_path, 'w') as wav_file:
        wav_file.setnchannels(1)
        wav_file.setsampwidth(2)
        wav_file.setframerate(sample_rate)
        wav_file.writeframes(audio_int16.tobytes())

    print(f"Created test audio: {wav_path}")

# Save test metadata
metadata = {
    "test_scenarios": test_scenarios,
    "audio_config": {
        "sample_rate": sample_rate,
        "channels": 1,
        "format": "16-bit PCM"
    },
    "created_at": "$(date -Iseconds)"
}

with open(os.path.join(validation_dir, "test_metadata.json"), 'w') as f:
    json.dump(metadata, f, indent=2)

print("Test metadata saved")
EOF

    log_success "âœ… Comprehensive test audio created"
else
    log_warn "âš ï¸  Python3 not available - using simple audio generation"

    # Create basic test audio using system tools if available
    if command -v sox >/dev/null 2>&1; then
        sox -n "$VALIDATION_DIR/simple_test.wav" synth 3 sine 440 vol 0.3
        log_info "   Created simple test audio using sox"
    fi
fi

# Validation 3: Comprehensive WebSocket pipeline test
log_info "ðŸ”„ Validation 3: Comprehensive WebSocket Pipeline Test"

cat > "$VALIDATION_DIR/comprehensive_test.py" << 'EOF'
#!/usr/bin/env python3
"""
Comprehensive end-to-end validation of WebSocket bridge
Tests real audio processing through the complete pipeline
"""

import asyncio
import websockets
import json
import time
import wave
import sys
import os
from pathlib import Path
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class WebSocketBridgeValidator:
    def __init__(self, server_url, validation_dir):
        self.server_url = server_url
        self.validation_dir = Path(validation_dir)
        self.results = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "test_details": []
        }

    async def validate_connection_lifecycle(self):
        """Test WebSocket connection establishment and teardown"""
        logger.info("Testing connection lifecycle...")

        try:
            async with websockets.connect(self.server_url) as websocket:
                # Test connection message
                msg = await asyncio.wait_for(websocket.recv(), timeout=10)
                data = json.loads(msg)

                if data.get('type') == 'connection':
                    connection_id = data.get('connection_id')
                    server_config = data.get('server_config', {})

                    logger.info(f"Connection established: {connection_id}")
                    logger.info(f"Server config: {server_config}")

                    # Validate server configuration
                    required_config = ['sample_rate', 'channels', 'frame_ms', 'riva_target']
                    config_valid = all(key in server_config for key in required_config)

                    self.record_test("connection_lifecycle", config_valid,
                                   f"Connection ID: {connection_id}, Config valid: {config_valid}")
                    return config_valid
                else:
                    self.record_test("connection_lifecycle", False, f"Invalid connection message: {data}")
                    return False

        except Exception as e:
            logger.error(f"Connection lifecycle test failed: {e}")
            self.record_test("connection_lifecycle", False, f"Exception: {e}")
            return False

    async def validate_transcription_session(self, audio_file=None):
        """Test complete transcription session with audio"""
        logger.info(f"Testing transcription session with audio: {audio_file}")

        try:
            async with websockets.connect(self.server_url) as websocket:
                # Wait for connection
                await websocket.recv()

                # Start transcription session
                start_msg = {
                    "type": "start_transcription",
                    "enable_partials": True,
                    "hotwords": ["test", "hello", "world", "machine", "learning"]
                }
                await websocket.send(json.dumps(start_msg))

                # Wait for session started
                session_msg = await asyncio.wait_for(websocket.recv(), timeout=10)
                session_data = json.loads(session_msg)

                if session_data.get('type') != 'session_started':
                    self.record_test("transcription_session", False,
                                   f"Session start failed: {session_data}")
                    return False

                logger.info("Transcription session started")

                # Send audio data
                audio_sent = False
                if audio_file and os.path.exists(audio_file):
                    with wave.open(audio_file, 'rb') as wav:
                        frames = wav.readframes(-1)

                        # Send audio in realistic chunks
                        chunk_size = 1600  # ~100ms at 16kHz
                        total_chunks = 0

                        for i in range(0, len(frames), chunk_size):
                            chunk = frames[i:i + chunk_size]
                            await websocket.send(chunk)
                            total_chunks += 1
                            await asyncio.sleep(0.1)  # Real-time simulation

                        audio_sent = True
                        logger.info(f"Sent {total_chunks} audio chunks ({len(frames)} bytes total)")

                # Collect transcription results
                partial_results = []
                final_results = []
                error_count = 0

                start_time = time.time()
                timeout_time = start_time + 20  # 20 second timeout

                while time.time() < timeout_time:
                    try:
                        msg = await asyncio.wait_for(websocket.recv(), timeout=5)
                        data = json.loads(msg)
                        msg_type = data.get('type')

                        if msg_type == 'partial':
                            partial_results.append(data)
                            logger.info(f"Partial: {data.get('text', '')[:50]}...")

                        elif msg_type == 'transcription':
                            final_results.append(data)
                            logger.info(f"Final: {data.get('text', '')} (conf: {data.get('confidence', 'N/A')})")

                        elif msg_type == 'error':
                            error_count += 1
                            logger.error(f"Transcription error: {data.get('error', '')}")

                    except asyncio.TimeoutError:
                        if audio_sent:
                            break  # Normal timeout after sending audio
                        else:
                            logger.warning("Timeout waiting for transcription results")
                            break

                # Stop transcription session
                stop_msg = {"type": "stop_transcription"}
                await websocket.send(json.dumps(stop_msg))

                try:
                    stop_response = await asyncio.wait_for(websocket.recv(), timeout=5)
                    stop_data = json.loads(stop_response)
                    session_stopped = stop_data.get('type') == 'session_stopped'
                except asyncio.TimeoutError:
                    session_stopped = False

                # Evaluate results
                total_results = len(partial_results) + len(final_results)
                has_transcription = len(final_results) > 0
                low_error_rate = error_count < 3

                success = audio_sent and has_transcription and low_error_rate and session_stopped

                details = {
                    "audio_sent": audio_sent,
                    "partial_results": len(partial_results),
                    "final_results": len(final_results),
                    "errors": error_count,
                    "session_stopped": session_stopped,
                    "duration": time.time() - start_time
                }

                self.record_test(f"transcription_session_{Path(audio_file).stem if audio_file else 'synthetic'}",
                               success, json.dumps(details))

                return success

        except Exception as e:
            logger.error(f"Transcription session test failed: {e}")
            self.record_test("transcription_session", False, f"Exception: {e}")
            return False

    async def validate_concurrent_sessions(self, num_sessions=3):
        """Test multiple concurrent transcription sessions"""
        logger.info(f"Testing {num_sessions} concurrent sessions...")

        async def single_session(session_id):
            try:
                async with websockets.connect(self.server_url) as websocket:
                    # Connection and start session
                    await websocket.recv()  # Connection message

                    start_msg = {"type": "start_transcription", "enable_partials": False}
                    await websocket.send(json.dumps(start_msg))

                    session_msg = await asyncio.wait_for(websocket.recv(), timeout=10)
                    session_data = json.loads(session_msg)

                    if session_data.get('type') != 'session_started':
                        return False

                    # Send some synthetic audio
                    for _ in range(10):
                        fake_audio = b'\\x00' * 1600  # Silence
                        await websocket.send(fake_audio)
                        await asyncio.sleep(0.1)

                    # Stop session
                    stop_msg = {"type": "stop_transcription"}
                    await websocket.send(json.dumps(stop_msg))

                    await asyncio.wait_for(websocket.recv(), timeout=5)  # Stop confirmation

                    logger.info(f"Session {session_id} completed successfully")
                    return True

            except Exception as e:
                logger.error(f"Session {session_id} failed: {e}")
                return False

        # Run concurrent sessions
        start_time = time.time()
        tasks = [single_session(i) for i in range(num_sessions)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        duration = time.time() - start_time

        successful_sessions = sum(1 for r in results if r is True)
        success = successful_sessions == num_sessions

        details = {
            "requested_sessions": num_sessions,
            "successful_sessions": successful_sessions,
            "duration": duration,
            "avg_duration_per_session": duration / num_sessions
        }

        self.record_test("concurrent_sessions", success, json.dumps(details))
        logger.info(f"Concurrent sessions: {successful_sessions}/{num_sessions} successful")

        return success

    async def validate_error_handling(self):
        """Test error handling and recovery"""
        logger.info("Testing error handling...")

        try:
            async with websockets.connect(self.server_url) as websocket:
                await websocket.recv()  # Connection message

                # Test invalid message
                await websocket.send("invalid json")

                # Should receive error message
                error_msg = await asyncio.wait_for(websocket.recv(), timeout=5)
                error_data = json.loads(error_msg)

                got_error_response = error_data.get('type') == 'error'

                # Test recovery - send valid message after error
                ping_msg = {"type": "ping", "timestamp": time.time()}
                await websocket.send(json.dumps(ping_msg))

                pong_msg = await asyncio.wait_for(websocket.recv(), timeout=5)
                pong_data = json.loads(pong_msg)

                recovered = pong_data.get('type') == 'pong'

                success = got_error_response and recovered

                details = {
                    "error_response_received": got_error_response,
                    "recovery_successful": recovered
                }

                self.record_test("error_handling", success, json.dumps(details))
                return success

        except Exception as e:
            logger.error(f"Error handling test failed: {e}")
            self.record_test("error_handling", False, f"Exception: {e}")
            return False

    async def validate_metrics_and_monitoring(self):
        """Test metrics and monitoring functionality"""
        logger.info("Testing metrics and monitoring...")

        try:
            async with websockets.connect(self.server_url) as websocket:
                await websocket.recv()  # Connection message

                # Request metrics
                metrics_msg = {"type": "get_metrics"}
                await websocket.send(json.dumps(metrics_msg))

                # Test ping/pong
                ping_msg = {"type": "ping", "timestamp": time.time()}
                await websocket.send(json.dumps(ping_msg))

                # Collect responses
                metrics_received = False
                pong_received = False

                for _ in range(3):  # Expect up to 3 messages
                    try:
                        msg = await asyncio.wait_for(websocket.recv(), timeout=5)
                        data = json.loads(msg)
                        msg_type = data.get('type')

                        if msg_type == 'metrics':
                            metrics_received = True
                            logger.info("Received metrics response")

                        elif msg_type == 'pong':
                            pong_received = True
                            logger.info("Received pong response")

                    except asyncio.TimeoutError:
                        break

                success = metrics_received and pong_received

                details = {
                    "metrics_received": metrics_received,
                    "pong_received": pong_received
                }

                self.record_test("metrics_monitoring", success, json.dumps(details))
                return success

        except Exception as e:
            logger.error(f"Metrics test failed: {e}")
            self.record_test("metrics_monitoring", False, f"Exception: {e}")
            return False

    def record_test(self, test_name, passed, details=""):
        """Record test result"""
        self.results["total_tests"] += 1
        if passed:
            self.results["passed_tests"] += 1
        else:
            self.results["failed_tests"] += 1

        self.results["test_details"].append({
            "test_name": test_name,
            "passed": passed,
            "details": details,
            "timestamp": time.time()
        })

    async def run_validation(self):
        """Run complete validation suite"""
        logger.info("Starting comprehensive WebSocket bridge validation...")

        # Test 1: Connection lifecycle
        await self.validate_connection_lifecycle()

        # Test 2: Find audio files and test transcription
        audio_files = list(self.validation_dir.glob("*.wav"))
        if audio_files:
            for audio_file in audio_files[:3]:  # Test up to 3 audio files
                await self.validate_transcription_session(str(audio_file))
        else:
            # Test with synthetic audio
            await self.validate_transcription_session()

        # Test 3: Concurrent sessions
        await self.validate_concurrent_sessions(3)

        # Test 4: Error handling
        await self.validate_error_handling()

        # Test 5: Metrics and monitoring
        await self.validate_metrics_and_monitoring()

        return self.results

def main():
    if len(sys.argv) < 3:
        print("Usage: python comprehensive_test.py <server_url> <validation_dir>")
        sys.exit(1)

    server_url = sys.argv[1]
    validation_dir = sys.argv[2]

    validator = WebSocketBridgeValidator(server_url, validation_dir)
    results = asyncio.run(validator.run_validation())

    # Print results
    print(f"\nðŸ“Š Validation Results:")
    print(f"   Total tests: {results['total_tests']}")
    print(f"   Passed: {results['passed_tests']}")
    print(f"   Failed: {results['failed_tests']}")
    print(f"   Success rate: {(results['passed_tests']/results['total_tests'])*100:.1f}%")

    print(f"\nðŸ“‹ Detailed Results:")
    for test in results['test_details']:
        status = "âœ… PASS" if test['passed'] else "âŒ FAIL"
        print(f"   {test['test_name']}: {status}")
        if test['details']:
            print(f"      {test['details'][:100]}...")

    # Save results
    results_file = os.path.join(validation_dir, "validation_results.json")
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nðŸ“„ Results saved to: {results_file}")

    # Exit with appropriate code
    if results['failed_tests'] == 0:
        print("\nðŸŽ‰ All validation tests passed!")
        sys.exit(0)
    else:
        print(f"\nâŒ {results['failed_tests']} validation tests failed!")
        sys.exit(1)

if __name__ == "__main__":
    main()
EOF

chmod +x "$VALIDATION_DIR/comprehensive_test.py"

# Run comprehensive validation
if command -v python3 >/dev/null 2>&1 && python3 -c "import websockets" 2>/dev/null; then
    log_info "Running comprehensive validation..."

    if python3 "$VALIDATION_DIR/comprehensive_test.py" "$SERVER_URL" "$VALIDATION_DIR"; then
        log_success "âœ… Comprehensive validation passed"
        COMPREHENSIVE_TEST_PASSED=true
    else
        log_error "âŒ Comprehensive validation failed"
        COMPREHENSIVE_TEST_PASSED=false
    fi
else
    log_warn "âš ï¸  Dependencies not available for comprehensive testing"
    COMPREHENSIVE_TEST_PASSED="skipped"
fi

# Validation 4: Performance and latency testing
log_info "âš¡ Validation 4: Performance and Latency Testing"

cat > "$VALIDATION_DIR/performance_test.py" << 'EOF'
#!/usr/bin/env python3
"""
Performance and latency testing for WebSocket bridge
"""

import asyncio
import websockets
import json
import time
import statistics
import sys

async def measure_connection_latency(server_url, num_tests=10):
    """Measure WebSocket connection establishment latency"""
    latencies = []

    for i in range(num_tests):
        start_time = time.time()
        try:
            async with websockets.connect(server_url) as websocket:
                await websocket.recv()  # Wait for connection message
                latency = (time.time() - start_time) * 1000  # Convert to ms
                latencies.append(latency)
                print(f"Connection {i+1}: {latency:.2f}ms")
        except Exception as e:
            print(f"Connection {i+1} failed: {e}")
            return None

        await asyncio.sleep(0.5)  # Brief pause between tests

    return latencies

async def measure_ping_latency(server_url, num_pings=20):
    """Measure ping/pong latency"""
    latencies = []

    try:
        async with websockets.connect(server_url) as websocket:
            await websocket.recv()  # Connection message

            for i in range(num_pings):
                start_time = time.time()
                ping_msg = {"type": "ping", "timestamp": start_time}
                await websocket.send(json.dumps(ping_msg))

                pong_msg = await asyncio.wait_for(websocket.recv(), timeout=5)
                latency = (time.time() - start_time) * 1000
                latencies.append(latency)

                print(f"Ping {i+1}: {latency:.2f}ms")
                await asyncio.sleep(0.2)

    except Exception as e:
        print(f"Ping test failed: {e}")
        return None

    return latencies

async def measure_throughput(server_url, duration=30):
    """Measure audio throughput"""
    messages_sent = 0
    bytes_sent = 0
    start_time = time.time()

    try:
        async with websockets.connect(server_url) as websocket:
            await websocket.recv()  # Connection message

            # Start transcription session
            start_msg = {"type": "start_transcription", "enable_partials": False}
            await websocket.send(json.dumps(start_msg))
            await websocket.recv()  # Session started

            # Send audio data for specified duration
            chunk_size = 1600  # 100ms of 16kHz audio
            fake_audio = b'\\x00' * chunk_size

            while (time.time() - start_time) < duration:
                await websocket.send(fake_audio)
                messages_sent += 1
                bytes_sent += len(fake_audio)
                await asyncio.sleep(0.1)  # 100ms intervals

            # Stop session
            stop_msg = {"type": "stop_transcription"}
            await websocket.send(json.dumps(stop_msg))

    except Exception as e:
        print(f"Throughput test failed: {e}")
        return None

    actual_duration = time.time() - start_time
    throughput_mbps = (bytes_sent * 8) / (actual_duration * 1024 * 1024)
    message_rate = messages_sent / actual_duration

    return {
        "duration": actual_duration,
        "messages_sent": messages_sent,
        "bytes_sent": bytes_sent,
        "throughput_mbps": throughput_mbps,
        "message_rate": message_rate
    }

def analyze_latencies(latencies, test_name):
    """Analyze latency statistics"""
    if not latencies:
        return None

    return {
        "test_name": test_name,
        "count": len(latencies),
        "min_ms": min(latencies),
        "max_ms": max(latencies),
        "mean_ms": statistics.mean(latencies),
        "median_ms": statistics.median(latencies),
        "stdev_ms": statistics.stdev(latencies) if len(latencies) > 1 else 0,
        "p95_ms": sorted(latencies)[int(len(latencies) * 0.95)],
        "p99_ms": sorted(latencies)[int(len(latencies) * 0.99)]
    }

async def main():
    if len(sys.argv) < 2:
        print("Usage: python performance_test.py <server_url>")
        sys.exit(1)

    server_url = sys.argv[1]

    print("ðŸš€ WebSocket Bridge Performance Testing")
    print(f"Server: {server_url}")
    print()

    # Test 1: Connection latency
    print("Test 1: Connection Latency")
    conn_latencies = await measure_connection_latency(server_url, 10)
    conn_stats = analyze_latencies(conn_latencies, "connection_latency")

    # Test 2: Ping latency
    print("\nTest 2: Ping/Pong Latency")
    ping_latencies = await measure_ping_latency(server_url, 20)
    ping_stats = analyze_latencies(ping_latencies, "ping_latency")

    # Test 3: Throughput
    print("\nTest 3: Audio Throughput (30 seconds)")
    throughput_stats = await measure_throughput(server_url, 30)

    # Results summary
    print(f"\nðŸ“Š Performance Test Results:")

    if conn_stats:
        print(f"\nConnection Latency:")
        print(f"  Mean: {conn_stats['mean_ms']:.2f}ms")
        print(f"  Median: {conn_stats['median_ms']:.2f}ms")
        print(f"  95th percentile: {conn_stats['p95_ms']:.2f}ms")
        print(f"  Max: {conn_stats['max_ms']:.2f}ms")

    if ping_stats:
        print(f"\nPing Latency:")
        print(f"  Mean: {ping_stats['mean_ms']:.2f}ms")
        print(f"  Median: {ping_stats['median_ms']:.2f}ms")
        print(f"  95th percentile: {ping_stats['p95_ms']:.2f}ms")
        print(f"  Std Dev: {ping_stats['stdev_ms']:.2f}ms")

    if throughput_stats:
        print(f"\nThroughput:")
        print(f"  Messages/second: {throughput_stats['message_rate']:.1f}")
        print(f"  Throughput: {throughput_stats['throughput_mbps']:.3f} Mbps")
        print(f"  Total bytes: {throughput_stats['bytes_sent']:,}")

    # Performance assessment
    performance_good = True
    issues = []

    if conn_stats and conn_stats['mean_ms'] > 1000:
        performance_good = False
        issues.append("High connection latency")

    if ping_stats and ping_stats['mean_ms'] > 100:
        performance_good = False
        issues.append("High ping latency")

    if throughput_stats and throughput_stats['message_rate'] < 8:
        performance_good = False
        issues.append("Low message throughput")

    print(f"\nðŸŽ¯ Performance Assessment: {'âœ… GOOD' if performance_good else 'âš ï¸ ISSUES'}")
    if issues:
        for issue in issues:
            print(f"  - {issue}")

    # Save results
    results = {
        "connection_latency": conn_stats,
        "ping_latency": ping_stats,
        "throughput": throughput_stats,
        "performance_assessment": {
            "overall_good": performance_good,
            "issues": issues
        },
        "timestamp": time.time()
    }

    import os
    validation_dir = os.path.dirname(__file__)
    results_file = os.path.join(validation_dir, "performance_results.json")

    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nðŸ“„ Results saved to: {results_file}")

    return performance_good

if __name__ == "__main__":
    result = asyncio.run(main())
    sys.exit(0 if result else 1)
EOF

chmod +x "$VALIDATION_DIR/performance_test.py"

# Run performance testing
if command -v python3 >/dev/null 2>&1 && python3 -c "import websockets" 2>/dev/null; then
    log_info "Running performance validation..."

    if python3 "$VALIDATION_DIR/performance_test.py" "$SERVER_URL"; then
        log_success "âœ… Performance validation passed"
        PERFORMANCE_TEST_PASSED=true
    else
        log_warn "âš ï¸  Performance validation has issues"
        PERFORMANCE_TEST_PASSED=false
    fi
else
    log_warn "âš ï¸  Dependencies not available for performance testing"
    PERFORMANCE_TEST_PASSED="skipped"
fi

# Validation 5: System resource monitoring
log_info "ðŸ“Š Validation 5: System Resource Monitoring"

# Monitor system resources during operation
RESOURCE_LOG="$VALIDATION_DIR/resource_monitoring.log"

log_info "Monitoring system resources for 60 seconds..."

{
    echo "Timestamp,CPU%,Memory%,WebSocket_Service_CPU%,WebSocket_Service_Memory%"

    for i in {1..12}; do  # 12 samples over 60 seconds
        timestamp=$(date '+%Y-%m-%d %H:%M:%S')

        # Overall system metrics
        cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
        memory_usage=$(free | grep Mem | awk '{printf "%.1f", ($3/$2) * 100.0}')

        # WebSocket service specific metrics
        service_cpu=""
        service_memory=""

        if pgrep -f "riva_websocket_bridge" >/dev/null; then
            service_pid=$(pgrep -f "riva_websocket_bridge")
            if [[ -n "$service_pid" ]]; then
                service_stats=$(ps -p "$service_pid" -o %cpu,%mem --no-headers 2>/dev/null || echo "0.0 0.0")
                service_cpu=$(echo "$service_stats" | awk '{print $1}')
                service_memory=$(echo "$service_stats" | awk '{print $2}')
            fi
        fi

        echo "$timestamp,$cpu_usage,$memory_usage,$service_cpu,$service_memory"

        sleep 5
    done
} > "$RESOURCE_LOG"

log_info "âœ… Resource monitoring completed - data saved to $RESOURCE_LOG"

# Analyze resource usage
if [[ -f "$RESOURCE_LOG" ]]; then
    avg_service_cpu=$(tail -n +2 "$RESOURCE_LOG" | awk -F',' '{sum+=$4; count++} END {if(count>0) print sum/count; else print 0}')
    avg_service_memory=$(tail -n +2 "$RESOURCE_LOG" | awk -F',' '{sum+=$5; count++} END {if(count>0) print sum/count; else print 0}')

    log_info "ðŸ“ˆ Average resource usage:"
    log_info "   WebSocket service CPU: ${avg_service_cpu}%"
    log_info "   WebSocket service Memory: ${avg_service_memory}%"

    # Resource usage assessment
    if (( $(echo "$avg_service_cpu < 50" | bc -l) )) && (( $(echo "$avg_service_memory < 10" | bc -l) )); then
        log_success "âœ… Resource usage within acceptable limits"
        RESOURCE_TEST_PASSED=true
    else
        log_warn "âš ï¸  High resource usage detected"
        RESOURCE_TEST_PASSED=false
    fi
else
    log_warn "âš ï¸  Resource monitoring data not available"
    RESOURCE_TEST_PASSED="skipped"
fi

# Generate comprehensive validation report
log_info "ðŸ“‹ Generating comprehensive validation report..."

cat > "$VALIDATION_DIR/validation_report.md" << EOF
# WebSocket Bridge End-to-End Validation Report

Generated: $(date -Iseconds)
Server: $SERVER_URL
Riva Target: $RIVA_HOST:$RIVA_PORT

## Executive Summary

This report documents the comprehensive end-to-end validation of the NVIDIA Riva WebSocket Bridge deployment.

## Infrastructure Validation

âœ… **Riva Server**: $RIVA_HOST:$RIVA_PORT accessible
âœ… **WebSocket Service**: Running and responsive on port $WS_PORT
âœ… **TLS Configuration**: $(if [[ "${WS_TLS_ENABLED}" == "true" ]]; then echo "Enabled"; else echo "Disabled"; fi)

## Test Results

| Test Category | Status | Details |
|---------------|--------|---------|
| Infrastructure Health | âœ… PASS | All core services operational |
| Comprehensive Testing | $(if [[ "$COMPREHENSIVE_TEST_PASSED" == "true" ]]; then echo "âœ… PASS"; elif [[ "$COMPREHENSIVE_TEST_PASSED" == "skipped" ]]; then echo "â­ï¸ SKIPPED"; else echo "âŒ FAIL"; fi) | Full pipeline validation |
| Performance Testing | $(if [[ "$PERFORMANCE_TEST_PASSED" == "true" ]]; then echo "âœ… PASS"; elif [[ "$PERFORMANCE_TEST_PASSED" == "skipped" ]]; then echo "â­ï¸ SKIPPED"; else echo "âš ï¸ ISSUES"; fi) | Latency and throughput validation |
| Resource Monitoring | $(if [[ "$RESOURCE_TEST_PASSED" == "true" ]]; then echo "âœ… PASS"; elif [[ "$RESOURCE_TEST_PASSED" == "skipped" ]]; then echo "â­ï¸ SKIPPED"; else echo "âš ï¸ HIGH USAGE"; fi) | System resource utilization |

## Detailed Findings

### Audio Processing Pipeline
- Sample Rate: ${WS_SAMPLE_RATE}Hz
- Channels: ${WS_CHANNELS}
- Frame Duration: ${WS_FRAME_MS}ms
- Partial Results: $(if [[ "${WS_PARTIAL_INTERVAL_MS}" -lt 500 ]]; then echo "Enabled (${WS_PARTIAL_INTERVAL_MS}ms)"; else echo "Disabled"; fi)

### Performance Metrics
$(if [[ -f "$VALIDATION_DIR/performance_results.json" ]]; then
    echo "- Connection Latency: Available in performance_results.json"
    echo "- Ping Latency: Available in performance_results.json"
    echo "- Throughput: Available in performance_results.json"
else
    echo "- Performance metrics not available"
fi)

### Resource Utilization
$(if [[ -f "$RESOURCE_LOG" ]]; then
    echo "- CPU Usage: ${avg_service_cpu}% (average)"
    echo "- Memory Usage: ${avg_service_memory}% (average)"
    echo "- Monitoring Duration: 60 seconds"
else
    echo "- Resource monitoring data not available"
fi)

## Validation Files

- **Test Audio**: $(ls "$VALIDATION_DIR"/*.wav 2>/dev/null | wc -l) files created
- **Comprehensive Test**: $VALIDATION_DIR/comprehensive_test.py
- **Performance Test**: $VALIDATION_DIR/performance_test.py
- **Resource Log**: $RESOURCE_LOG
- **Results**: JSON files with detailed metrics

## Recommendations

### Production Readiness
$(if [[ "$COMPREHENSIVE_TEST_PASSED" == "true" && "$PERFORMANCE_TEST_PASSED" == "true" ]]; then
    echo "âœ… **READY FOR PRODUCTION** - All core tests passed"
else
    echo "âš ï¸ **NEEDS ATTENTION** - Review failed tests before production deployment"
fi)

### Monitoring
- Set up continuous health checks using: \`/opt/riva/health-check-websocket-bridge.sh\`
- Monitor service logs: \`sudo journalctl -u riva-websocket-bridge.service -f\`
- Track resource usage and set up alerts for high CPU/memory usage

### Scaling Considerations
- Current configuration supports up to ${WS_MAX_CONNECTIONS} concurrent connections
- Consider load balancer for high-traffic scenarios
- Monitor Riva server capacity as the bottleneck

## Conclusion

$(if [[ "$COMPREHENSIVE_TEST_PASSED" == "true" ]]; then
    echo "The WebSocket bridge deployment has been successfully validated and is ready for production use."
else
    echo "The WebSocket bridge deployment requires attention to address validation failures before production use."
fi)

---
*This report was automatically generated by riva-144-end-to-end-validation.sh*
EOF

log_success "âœ… Validation report generated: $VALIDATION_DIR/validation_report.md"

# Update validation status
sudo tee -a /opt/riva/nvidia-parakeet-ver-6/.env > /dev/null << EOF

# End-to-End Validation Results (Updated by riva-144)
WS_E2E_VALIDATION_COMPLETE=true
WS_E2E_VALIDATION_TIMESTAMP=$(date -Iseconds)
WS_COMPREHENSIVE_TEST_PASSED=${COMPREHENSIVE_TEST_PASSED}
WS_PERFORMANCE_TEST_PASSED=${PERFORMANCE_TEST_PASSED}
WS_RESOURCE_TEST_PASSED=${RESOURCE_TEST_PASSED}
EOF

# Display validation summary
echo
log_info "ðŸ“‹ End-to-End Validation Summary:"
echo "   Server URL: $SERVER_URL"
echo "   Validation Directory: $VALIDATION_DIR"
echo
echo "   Test Results:"
echo "     Infrastructure: âœ… PASS"
echo "     Comprehensive: $(if [[ "$COMPREHENSIVE_TEST_PASSED" == "true" ]]; then echo "âœ… PASS"; elif [[ "$COMPREHENSIVE_TEST_PASSED" == "skipped" ]]; then echo "â­ï¸ SKIPPED"; else echo "âŒ FAIL"; fi)"
echo "     Performance: $(if [[ "$PERFORMANCE_TEST_PASSED" == "true" ]]; then echo "âœ… PASS"; elif [[ "$PERFORMANCE_TEST_PASSED" == "skipped" ]]; then echo "â­ï¸ SKIPPED"; else echo "âš ï¸ ISSUES"; fi)"
echo "     Resources: $(if [[ "$RESOURCE_TEST_PASSED" == "true" ]]; then echo "âœ… PASS"; elif [[ "$RESOURCE_TEST_PASSED" == "skipped" ]]; then echo "â­ï¸ SKIPPED"; else echo "âš ï¸ HIGH"; fi)"

# Overall assessment
OVERALL_VALIDATION_SUCCESS=true
if [[ "$COMPREHENSIVE_TEST_PASSED" == "false" || "$PERFORMANCE_TEST_PASSED" == "false" ]]; then
    OVERALL_VALIDATION_SUCCESS=false
fi

echo
if [[ "$OVERALL_VALIDATION_SUCCESS" == "true" ]]; then
    log_success "ðŸŽ‰ End-to-end validation completed successfully!"
    echo "   The WebSocket bridge is ready for production use."
else
    log_warn "âš ï¸  End-to-end validation completed with issues"
    echo "   Review the validation report and address any failures before production."
fi

echo
echo "ðŸ“„ Validation Report: $VALIDATION_DIR/validation_report.md"
echo "ðŸ“ Test Files: $VALIDATION_DIR/"
echo
echo "Next steps:"
echo "  1. Run: ./scripts/riva-145-production-health-checks.sh"
echo "  2. Review: $VALIDATION_DIR/validation_report.md"
echo "  3. Monitor: sudo journalctl -u riva-websocket-bridge.service -f"