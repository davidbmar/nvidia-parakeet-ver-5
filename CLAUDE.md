# NVIDIA Parakeet RNNT via Riva ASR - Implementation Plan

## ğŸ“‹ Current Status (Updated 2025-09-07)

âœ… **M0 â€“ Plan Locked**: Architecture mapped, ASR boundaries identified  
âœ… **M1 â€“ Riva Online**: NIM/Traditional Riva containers deployed with health checks  
âœ… **M2 â€“ Client Wrapper**: `src/asr/riva_client.py` implemented (665 lines) with streaming support  
ğŸ”„ **M3 â€“ WS Integration**: WebSocket server exists, needs real Riva integration (mock mode ready)  
â³ **M4 â€“ Observability**: Basic logging in place, metrics implementation pending  
â³ **M5 â€“ Production Ready**: Security hardening and deployment validation pending  

## ğŸš€ Deployment Infrastructure Complete

- **60+ Scripts**: Complete deployment automation in `scripts/riva-*`
- **NIM + Traditional**: Both modern NIM containers and traditional Riva server support
- **AWS Integration**: Full EC2 GPU instance deployment with driver automation
- **Comprehensive Testing**: File transcription, streaming, end-to-end validation scripts

---

# ğŸ›  Development Best Practices & Standards

## ğŸ”§ Configuration & Environment Management

â€¢ **Always use .env file** - Never hardcode configuration values
â€¢ **Start with .env.example** as template, copy to .env for local development
â€¢ **Ask users to configure secure values** (API keys, passwords, certificates) interactively
â€¢ **Never commit .env file** - keep it in .gitignore, only commit .env.example
â€¢ **Use environment variables as primary config source** with pydantic validation
â€¢ **Prompt for missing critical config** during setup scripts

## ğŸ“ Script Organization & Execution Order

â€¢ **Number scripts sequentially** as `./scripts/riva-XXX-description.sh` where XXX is execution order
â€¢ **Design scripts for first-time checkout** - assume clean environment
â€¢ **Make scripts idempotent** - safe to run multiple times
â€¢ **Include prerequisite checks** at start of each script
â€¢ **Chain scripts logically** - each script sets up for the next one
â€¢ **Document execution order** in README or main script

## ğŸ“ Comprehensive Logging Strategy

â€¢ **Log everything in scripts** - timestamps, actions, results, errors
â€¢ **Use structured log format** with consistent timestamp and level indicators
â€¢ **Write logs to files** AND console output for debugging
â€¢ **Include enough detail** so AI can diagnose issues from logs alone
â€¢ **Log environment state** before/after major operations
â€¢ **Use color coding** for different log levels (ERROR=red, SUCCESS=green, etc.)

## ğŸ”„ Step-by-Step Development Process

â€¢ **Think smallest possible step** - break down complex tasks into atomic operations
â€¢ **Test manually first** - verify each step works by hand/command line
â€¢ **Implement in code** - write the actual implementation
â€¢ **Create test script** - automate validation of the implementation
â€¢ **Write validation script** - verify desired end state was reached
â€¢ **Have user execute** - run implementation script then validation script
â€¢ **Number appropriately** - place in correct sequence for first-time setup

## ğŸ›¡ Error Handling & Debugging

â€¢ **Log all errors with context** - what was being attempted, environment state
â€¢ **Include retry logic** with logged attempts
â€¢ **Validate prerequisites** and log missing dependencies
â€¢ **Create detailed error messages** that point to likely solutions
â€¢ **Log performance metrics** (timing, resource usage)
â€¢ **Include stack traces** in debug logs

## ğŸ”’ Security & Configuration Safety

â€¢ **Prompt for sensitive values** - don't put in example files
â€¢ **Validate configuration** before proceeding with operations
â€¢ **Use secure defaults** where possible
â€¢ **Warn about insecure configurations** in development vs production
â€¢ **Encrypt sensitive data** at rest when possible

## ğŸ“Š Production Readiness Checklist

â€¢ **Health checks in every component** - can be monitored by ops teams
â€¢ **Graceful degradation** - system works even when dependencies are down
â€¢ **Resource cleanup** - scripts clean up after themselves on failure
â€¢ **Rollback capability** - ability to undo script actions
â€¢ **Monitoring integration** - logs can be ingested by monitoring systems

## ğŸ“œ Script Template Structure

```bash
#!/bin/bash
set -euo pipefail

# Script: riva-XXX-description.sh
# Purpose: What this script does
# Prerequisites: What needs to be done first
# Validation: How to verify success

source "$(dirname "$0")/riva-common-functions.sh"
load_config

log_info "ğŸš€ Starting XXX operation..."
validate_prerequisites
perform_operation
validate_results  
log_success "âœ… XXX operation completed successfully"
```

## ğŸ’» Code Style & Structure

â€¢ **Always use comprehensive type hints** with `typing` module (`Dict[str, Any]`, `Optional[str]`, `AsyncGenerator`)
â€¢ **Write Google-style docstrings** with Args, Returns, and Yields sections
â€¢ **Use async/await consistently** throughout - this is an async-first codebase
â€¢ **Organize code in classes** with clear single responsibilities
â€¢ **Use enums for constants** instead of magic strings/numbers
â€¢ **Use dataclasses for configuration** objects

## âš ï¸ Error Handling Patterns

â€¢ **Implement layered error handling** - catch specific exceptions first, general ones last
â€¢ **Always handle gRPC errors specifically** with `grpc.RpcError`
â€¢ **Provide graceful degradation** with mock modes and fallbacks
â€¢ **Create standardized error response formats** for consistent client handling
â€¢ **Implement retry logic** with configurable parameters and exponential backoff

## ğŸ§ª Testing Philosophy

â€¢ **Test against real services** when possible, not just mocks
â€¢ **Include connection validation tests** before integration
â€¢ **Build in mock mode support** for offline development
â€¢ **Create end-to-end validation** with actual audio files
â€¢ **Use descriptive test file names** indicating purpose

---

# ğŸ¯ High-Level Goals

Stand up a Riva/NIM ASR instance exposing Parakeet RNNT over gRPC.

Replace the local RNNT path in your repo with a thin Riva client wrapper that preserves your current JSON/WS contract (partials/finals).

Ship observability, tests, and load checks so you can trust latency, accuracy, and stability in prod.

Harden for security & failure modes (TLS, timeouts, retries, backpressure).

